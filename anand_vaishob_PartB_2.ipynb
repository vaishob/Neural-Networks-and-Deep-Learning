{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFVxWZGJxprU"
   },
   "source": [
    "# Question B2 (10 marks)\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EycCozG06Duu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-widedeep\n",
      "  Downloading pytorch_widedeep-1.6.3-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas>=1.3.5 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from pytorch-widedeep) (2.1.4)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.21.6 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from pytorch-widedeep) (1.26.4)\n",
      "Requirement already satisfied: scipy<=1.12.0,>=1.7.3 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from pytorch-widedeep) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from pytorch-widedeep) (1.5.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from pytorch-widedeep) (4.3.0)\n",
      "Collecting spacy (from pytorch-widedeep)\n",
      "  Downloading spacy-3.8.2-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Collecting opencv-contrib-python (from pytorch-widedeep)\n",
      "  Downloading opencv_contrib_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting imutils (from pytorch-widedeep)\n",
      "  Downloading imutils-0.5.4.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tqdm in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from pytorch-widedeep) (4.65.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from pytorch-widedeep) (2.4.1)\n",
      "Collecting torchvision>=0.15.0 (from pytorch-widedeep)\n",
      "  Downloading torchvision-0.19.1-cp311-cp311-win_amd64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: einops in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from pytorch-widedeep) (0.7.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from pytorch-widedeep) (1.14.1)\n",
      "Requirement already satisfied: torchmetrics in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from pytorch-widedeep) (1.2.1)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from pytorch-widedeep) (14.0.2)\n",
      "Collecting fastparquet>=0.8.1 (from pytorch-widedeep)\n",
      "  Downloading fastparquet-2024.5.0-cp311-cp311-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting transformers (from pytorch-widedeep)\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.4/44.4 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting sentence-transformers (from pytorch-widedeep)\n",
      "  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sentencepiece (from pytorch-widedeep)\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting cramjam>=2.3 (from fastparquet>=0.8.1->pytorch-widedeep)\n",
      "  Downloading cramjam-2.8.4-cp311-none-win_amd64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (2023.10.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (3.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from torchvision>=0.15.0->pytorch-widedeep) (10.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from gensim->pytorch-widedeep) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim->pytorch-widedeep)\n",
      "  Downloading FuzzyTM-2.0.9-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers->pytorch-widedeep)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from transformers->pytorch-widedeep) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from transformers->pytorch-widedeep) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from transformers->pytorch-widedeep) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers->pytorch-widedeep)\n",
      "  Downloading safetensors-0.4.5-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers->pytorch-widedeep)\n",
      "  Downloading tokenizers-0.20.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from tqdm->pytorch-widedeep) (0.4.6)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy->pytorch-widedeep)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy->pytorch-widedeep)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy->pytorch-widedeep)\n",
      "  Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy->pytorch-widedeep)\n",
      "  Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy->pytorch-widedeep)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy->pytorch-widedeep)\n",
      "  Downloading thinc-8.3.2-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy->pytorch-widedeep)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy->pytorch-widedeep)\n",
      "  Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy->pytorch-widedeep)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy->pytorch-widedeep)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy->pytorch-widedeep)\n",
      "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from spacy->pytorch-widedeep) (1.10.12)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from spacy->pytorch-widedeep) (68.2.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy->pytorch-widedeep)\n",
      "  Downloading langcodes-3.4.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from torchmetrics->pytorch-widedeep) (0.11.7)\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim->pytorch-widedeep)\n",
      "  Downloading pyFUME-0.3.4-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy->pytorch-widedeep)\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->pytorch-widedeep) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from requests->transformers->pytorch-widedeep) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from requests->transformers->pytorch-widedeep) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from requests->transformers->pytorch-widedeep) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from requests->transformers->pytorch-widedeep) (2024.8.30)\n",
      "Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->spacy->pytorch-widedeep)\n",
      "  Downloading blis-1.0.1-cp311-cp311-win_amd64.whl.metadata (7.8 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->spacy->pytorch-widedeep)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy->pytorch-widedeep)\n",
      "  Downloading thinc-8.3.1-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "  Downloading thinc-8.3.0-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim->pytorch-widedeep)\n",
      "  Downloading FuzzyTM-2.0.8-py3-none-any.whl.metadata (7.9 kB)\n",
      "  Downloading FuzzyTM-2.0.7-py3-none-any.whl.metadata (7.9 kB)\n",
      "INFO: pip is still looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading FuzzyTM-2.0.6-py3-none-any.whl.metadata (7.9 kB)\n",
      "  Downloading FuzzyTM-2.0.5-py3-none-any.whl.metadata (7.8 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading FuzzyTM-2.0.4-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Downloading FuzzyTM-2.0.3-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Downloading FuzzyTM-2.0.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Downloading FuzzyTM-2.0.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "INFO: pip is looking at multiple versions of fuzzytm to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading FuzzyTM-1.0.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting gensim (from pytorch-widedeep)\n",
      "  Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl.metadata (8.2 kB)\n",
      "Collecting torchmetrics (from pytorch-widedeep)\n",
      "  Downloading torchmetrics-1.4.3-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading torchmetrics-1.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading torchmetrics-1.4.1-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading torchmetrics-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pretty-errors==1.2.25 (from torchmetrics->pytorch-widedeep)\n",
      "  Downloading pretty_errors-1.2.25-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting torchmetrics (from pytorch-widedeep)\n",
      "  Downloading torchmetrics-1.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading torchmetrics-1.3.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading torchmetrics-1.3.0.post0-py3-none-any.whl.metadata (20 kB)\n",
      "  Using cached torchmetrics-1.2.1-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading torchmetrics-1.2.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading torchmetrics-1.1.2-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading torchmetrics-1.1.1-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading torchmetrics-1.1.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading torchmetrics-1.0.3-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading torchmetrics-1.0.2-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading torchmetrics-1.0.1-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading torchmetrics-1.0.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading torchmetrics-0.11.4-py3-none-any.whl.metadata (15 kB)\n",
      "  Downloading torchmetrics-0.11.3-py3-none-any.whl.metadata (15 kB)\n",
      "  Downloading torchmetrics-0.11.2-py3-none-any.whl.metadata (15 kB)\n",
      "  Downloading torchmetrics-0.11.1-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading torchmetrics-0.11.0-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading torchmetrics-0.10.3-py3-none-any.whl.metadata (15 kB)\n",
      "  Downloading torchmetrics-0.10.2-py3-none-any.whl.metadata (15 kB)\n",
      "  Downloading torchmetrics-0.10.1-py3-none-any.whl.metadata (15 kB)\n",
      "  Downloading torchmetrics-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Downloading torchmetrics-0.9.3-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading torchmetrics-0.9.2-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading torchmetrics-0.9.1-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading torchmetrics-0.9.0-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading torchmetrics-0.8.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting pyDeprecate==0.3.* (from torchmetrics->pytorch-widedeep)\n",
      "  Downloading pyDeprecate-0.3.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting torchmetrics (from pytorch-widedeep)\n",
      "  Downloading torchmetrics-0.8.1-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading torchmetrics-0.8.0-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading torchmetrics-0.7.3-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading torchmetrics-0.7.2-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading torchmetrics-0.7.1-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading torchmetrics-0.7.0-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading torchmetrics-0.6.2-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading torchmetrics-0.6.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading torchmetrics-0.6.0-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading torchmetrics-0.5.1-py3-none-any.whl.metadata (15 kB)\n",
      "  Downloading torchmetrics-0.5.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Downloading torchmetrics-0.4.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading torchmetrics-0.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting spacy (from pytorch-widedeep)\n",
      "  Downloading spacy-3.7.5-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy->pytorch-widedeep)\n",
      "  Downloading thinc-8.2.5-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy->pytorch-widedeep)\n",
      "  Downloading blis-0.7.11-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (13.3.5)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy->pytorch-widedeep)\n",
      "  Downloading cloudpathlib-0.19.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->pytorch-widedeep) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from sympy->torch>=2.0.0->pytorch-widedeep) (1.3.0)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->pytorch-widedeep)\n",
      "  Downloading marisa_trie-1.2.0-cp311-cp311-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\vaishob\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (0.1.0)\n",
      "Downloading pytorch_widedeep-1.6.3-py3-none-any.whl (21.9 MB)\n",
      "   ---------------------------------------- 0.0/21.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.2/21.9 MB 7.6 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 1.0/21.9 MB 10.1 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 1.5/21.9 MB 10.8 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 2.5/21.9 MB 14.3 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 3.1/21.9 MB 14.1 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 3.7/21.9 MB 13.9 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 4.6/21.9 MB 14.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 5.4/21.9 MB 14.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 6.1/21.9 MB 14.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 6.8/21.9 MB 15.0 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 7.6/21.9 MB 15.2 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 8.5/21.9 MB 15.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 9.4/21.9 MB 15.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 10.3/21.9 MB 16.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 11.1/21.9 MB 17.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 11.8/21.9 MB 17.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 12.7/21.9 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 13.6/21.9 MB 17.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 14.3/21.9 MB 17.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 15.3/21.9 MB 18.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 16.0/21.9 MB 18.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 17.0/21.9 MB 18.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 17.7/21.9 MB 18.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 17.7/21.9 MB 18.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 17.7/21.9 MB 18.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 17.7/21.9 MB 18.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 17.7/21.9 MB 18.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 17.7/21.9 MB 18.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 17.8/21.9 MB 12.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 18.3/21.9 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 18.9/21.9 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 19.9/21.9 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 20.5/21.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  21.5/21.9 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  21.9/21.9 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 21.9/21.9 MB 11.1 MB/s eta 0:00:00\n",
      "Downloading fastparquet-2024.5.0-cp311-cp311-win_amd64.whl (671 kB)\n",
      "   ---------------------------------------- 0.0/672.0 kB ? eta -:--:--\n",
      "   --------------------------------------  665.6/672.0 kB 21.1 MB/s eta 0:00:01\n",
      "   --------------------------------------- 672.0/672.0 kB 14.4 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.19.1-cp311-cp311-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 0.7/1.3 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 20.6 MB/s eta 0:00:00\n",
      "Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/24.0 MB 16.8 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 1.8/24.0 MB 18.8 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 2.5/24.0 MB 17.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 3.2/24.0 MB 17.2 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 4.0/24.0 MB 17.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 5.0/24.0 MB 17.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 5.7/24.0 MB 17.4 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 6.5/24.0 MB 17.9 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 7.4/24.0 MB 17.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 8.4/24.0 MB 17.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 8.9/24.0 MB 17.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 9.9/24.0 MB 18.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 10.9/24.0 MB 18.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 11.6/24.0 MB 18.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.1/24.0 MB 17.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 12.7/24.0 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 13.3/24.0 MB 16.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 14.1/24.0 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 15.0/24.0 MB 16.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 15.9/24.0 MB 17.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 16.9/24.0 MB 17.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 17.6/24.0 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.4/24.0 MB 16.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 19.5/24.0 MB 17.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.4/24.0 MB 17.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.2/24.0 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.3/24.0 MB 18.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.2/24.0 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.0 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 16.8 MB/s eta 0:00:00\n",
      "Downloading opencv_contrib_python-4.10.0.84-cp37-abi3-win_amd64.whl (45.5 MB)\n",
      "   ---------------------------------------- 0.0/45.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.9/45.5 MB 27.7 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 1.5/45.5 MB 18.8 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 2.6/45.5 MB 20.9 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 3.5/45.5 MB 20.2 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 4.5/45.5 MB 20.5 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 5.4/45.5 MB 20.4 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 6.3/45.5 MB 20.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 7.0/45.5 MB 19.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 8.0/45.5 MB 19.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 8.6/45.5 MB 19.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 9.6/45.5 MB 19.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 10.5/45.5 MB 19.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 11.6/45.5 MB 19.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 12.5/45.5 MB 19.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 13.3/45.5 MB 19.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 14.2/45.5 MB 19.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 15.1/45.5 MB 18.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 16.0/45.5 MB 19.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 16.4/45.5 MB 18.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 17.3/45.5 MB 18.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 18.2/45.5 MB 18.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 19.1/45.5 MB 18.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 19.7/45.5 MB 17.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 20.0/45.5 MB 17.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 20.9/45.5 MB 17.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 21.7/45.5 MB 16.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 22.4/45.5 MB 16.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 23.4/45.5 MB 17.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 24.3/45.5 MB 16.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 24.9/45.5 MB 16.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 25.6/45.5 MB 16.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 26.6/45.5 MB 16.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 27.3/45.5 MB 16.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 28.2/45.5 MB 16.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 29.2/45.5 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 29.6/45.5 MB 16.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 30.6/45.5 MB 17.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 31.5/45.5 MB 17.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 31.9/45.5 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 32.8/45.5 MB 17.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 33.7/45.5 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 34.2/45.5 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 35.2/45.5 MB 17.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 35.6/45.5 MB 16.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 36.4/45.5 MB 16.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 37.2/45.5 MB 16.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 37.8/45.5 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 38.9/45.5 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 39.6/45.5 MB 16.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 40.3/45.5 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 41.1/45.5 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 41.7/45.5 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 42.5/45.5 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 43.2/45.5 MB 16.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 44.1/45.5 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.1/45.5 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.5/45.5 MB 15.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.5/45.5 MB 15.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 45.5/45.5 MB 13.9 MB/s eta 0:00:00\n",
      "Downloading sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n",
      "   ---------------------------------------- 0.0/255.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 255.2/255.2 kB 16.3 MB/s eta 0:00:00\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.9/9.9 MB 27.5 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.8/9.9 MB 22.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.6/9.9 MB 20.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.3/9.9 MB 18.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.1/9.9 MB 18.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.0/9.9 MB 20.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.8/9.9 MB 19.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.9/9.9 MB 19.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.6/9.9 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.6/9.9 MB 18.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.4/9.9 MB 18.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.9/9.9 MB 18.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.9/9.9 MB 18.0 MB/s eta 0:00:00\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   ---------------------------------- ---- 880.6/991.5 kB 28.1 MB/s eta 0:00:01\n",
      "   --------------------------------------- 991.5/991.5 kB 21.3 MB/s eta 0:00:00\n",
      "Downloading spacy-3.7.5-cp311-cp311-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/12.1 MB 17.6 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.6/12.1 MB 16.6 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.6/12.1 MB 18.3 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.3/12.1 MB 17.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.5/12.1 MB 19.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.0/12.1 MB 17.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.9/12.1 MB 18.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.4/12.1 MB 17.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.2/12.1 MB 17.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.1/12.1 MB 17.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.8/12.1 MB 17.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.9/12.1 MB 17.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.7/12.1 MB 17.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.4/12.1 MB 18.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 17.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 16.4 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.5-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 20.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 18.6 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cramjam-2.8.4-cp311-none-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.8/2.1 MB 24.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.1 MB 16.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.1 MB 16.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.1 MB 16.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.1 MB 16.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.1 MB 16.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.4/2.1 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 5.6 MB/s eta 0:00:00\n",
      "Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "   ---------------------------------------- 0.0/436.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 436.6/436.6 kB 13.8 MB/s eta 0:00:00\n",
      "Downloading langcodes-3.4.1-py3-none-any.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/182.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 182.4/182.4 kB 11.5 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.3/122.3 kB 7.5 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "   ---------------------------------------- 0.0/286.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 286.0/286.0 kB 17.2 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl (479 kB)\n",
      "   ---------------------------------------- 0.0/479.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 479.7/479.7 kB 31.3 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.20.1-cp311-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.8/2.4 MB 17.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.3/2.4 MB 21.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.2/2.4 MB 15.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 13.8 MB/s eta 0:00:00\n",
      "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.3/47.3 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.3/50.3 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading blis-0.7.11-cp311-cp311-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.6/6.6 MB 17.3 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.3/6.6 MB 16.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.2/6.6 MB 17.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.1/6.6 MB 17.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.1/6.6 MB 18.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 17.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.8/6.6 MB 18.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 18.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 17.6 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.19.0-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 0.0/49.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 49.4/49.4 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.4/5.4 MB 13.2 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.3/5.4 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.2/5.4 MB 17.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.0/5.4 MB 17.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.7/5.4 MB 16.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.8/5.4 MB 18.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.4/5.4 MB 18.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 17.2 MB/s eta 0:00:00\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.0-cp311-cp311-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.6/152.6 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: imutils\n",
      "  Building wheel for imutils (setup.py): started\n",
      "  Building wheel for imutils (setup.py): finished with status 'done'\n",
      "  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25855 sha256=1f21a59ec110ee41600504d9fd9958329346415fdc59338ebf47a819e07ae87c\n",
      "  Stored in directory: c:\\users\\vaishob\\appdata\\local\\pip\\cache\\wheels\\31\\d0\\2c\\87ce38f6052879e5b7b18f0f8b4a10ad2a9d210e908d449f16\n",
      "Successfully built imutils\n",
      "Installing collected packages: sentencepiece, imutils, cymem, wasabi, spacy-loggers, spacy-legacy, shellingham, safetensors, opencv-contrib-python, murmurhash, marisa-trie, cramjam, cloudpathlib, catalogue, blis, srsly, preshed, language-data, huggingface-hub, gensim, typer, torchvision, tokenizers, langcodes, fastparquet, confection, weasel, transformers, thinc, spacy, sentence-transformers, pytorch-widedeep\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.3.0\n",
      "    Uninstalling gensim-4.3.0:\n",
      "      Successfully uninstalled gensim-4.3.0\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.19.0 confection-0.1.5 cramjam-2.8.4 cymem-2.0.8 fastparquet-2024.5.0 gensim-4.3.3 huggingface-hub-0.25.2 imutils-0.5.4 langcodes-3.4.1 language-data-1.2.0 marisa-trie-1.2.0 murmurhash-1.0.10 opencv-contrib-python-4.10.0.84 preshed-3.0.9 pytorch-widedeep-1.6.3 safetensors-0.4.5 sentence-transformers-3.2.0 sentencepiece-0.2.0 shellingham-1.5.4 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 tokenizers-0.20.1 torchvision-0.19.1 transformers-4.45.2 typer-0.12.5 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-widedeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lq0elU0J53Yo"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.metrics import R2Score\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU3xdVpwzuLx"
   },
   "source": [
    "1.Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_oYG6lNIh7Mp"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Step 2: Train-Test Split\n",
    "# Using entries from year 2020 and before as training data, and entries from 2021 and after as test data\n",
    "train_data = df[df['year'] <= 2020]\n",
    "test_data = df[df['year'] >= 2021]\n",
    "\n",
    "# Drop unused columns\n",
    "train_data = train_data.drop(columns=['full_address', 'nearest_stn', 'year'])\n",
    "test_data = test_data.drop(columns=['full_address', 'nearest_stn', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_q9PoR50JAA"
   },
   "source": [
    "2.Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 100 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZBY1iqUXtYWn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vaishob\\anaconda3\\Lib\\site-packages\\pytorch_widedeep\\preprocessing\\tab_preprocessor.py:360: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|██████████| 1366/1366 [00:16<00:00, 80.42it/s, loss=1.87e+5, metrics={'r2': -1.3086}]\n",
      "epoch 2: 100%|██████████| 1366/1366 [00:17<00:00, 77.83it/s, loss=1.01e+5, metrics={'r2': 0.4743}]\n",
      "epoch 3: 100%|██████████| 1366/1366 [00:18<00:00, 74.25it/s, loss=8.26e+4, metrics={'r2': 0.6563}]\n",
      "epoch 4: 100%|██████████| 1366/1366 [00:17<00:00, 79.15it/s, loss=6.85e+4, metrics={'r2': 0.7795}]\n",
      "epoch 5: 100%|██████████| 1366/1366 [00:17<00:00, 77.24it/s, loss=6.25e+4, metrics={'r2': 0.8228}]\n",
      "epoch 6: 100%|██████████| 1366/1366 [00:17<00:00, 76.53it/s, loss=6e+4, metrics={'r2': 0.8381}]   \n",
      "epoch 7: 100%|██████████| 1366/1366 [00:18<00:00, 72.18it/s, loss=5.89e+4, metrics={'r2': 0.8447}]\n",
      "epoch 8: 100%|██████████| 1366/1366 [00:17<00:00, 79.70it/s, loss=5.79e+4, metrics={'r2': 0.8501}]\n",
      "epoch 9: 100%|██████████| 1366/1366 [00:17<00:00, 76.98it/s, loss=5.73e+4, metrics={'r2': 0.853}] \n",
      "epoch 10: 100%|██████████| 1366/1366 [00:18<00:00, 74.46it/s, loss=5.66e+4, metrics={'r2': 0.8564}]\n",
      "epoch 11: 100%|██████████| 1366/1366 [00:19<00:00, 71.32it/s, loss=5.59e+4, metrics={'r2': 0.8602}]\n",
      "epoch 12: 100%|██████████| 1366/1366 [00:18<00:00, 75.24it/s, loss=5.54e+4, metrics={'r2': 0.8629}]\n",
      "epoch 13: 100%|██████████| 1366/1366 [00:18<00:00, 75.31it/s, loss=5.48e+4, metrics={'r2': 0.8657}]\n",
      "epoch 14: 100%|██████████| 1366/1366 [00:18<00:00, 72.27it/s, loss=5.43e+4, metrics={'r2': 0.8683}]\n",
      "epoch 15: 100%|██████████| 1366/1366 [00:22<00:00, 60.49it/s, loss=5.38e+4, metrics={'r2': 0.8706}]\n",
      "epoch 16: 100%|██████████| 1366/1366 [00:20<00:00, 67.82it/s, loss=5.31e+4, metrics={'r2': 0.874}] \n",
      "epoch 17: 100%|██████████| 1366/1366 [00:20<00:00, 66.39it/s, loss=5.29e+4, metrics={'r2': 0.8752}]\n",
      "epoch 18: 100%|██████████| 1366/1366 [00:20<00:00, 67.89it/s, loss=5.23e+4, metrics={'r2': 0.8782}]\n",
      "epoch 19: 100%|██████████| 1366/1366 [00:20<00:00, 67.37it/s, loss=5.21e+4, metrics={'r2': 0.8794}]\n",
      "epoch 20: 100%|██████████| 1366/1366 [00:20<00:00, 65.54it/s, loss=5.16e+4, metrics={'r2': 0.8818}]\n",
      "epoch 21: 100%|██████████| 1366/1366 [00:20<00:00, 67.54it/s, loss=5.14e+4, metrics={'r2': 0.8828}]\n",
      "epoch 22: 100%|██████████| 1366/1366 [00:21<00:00, 64.24it/s, loss=5.13e+4, metrics={'r2': 0.883}] \n",
      "epoch 23: 100%|██████████| 1366/1366 [00:20<00:00, 66.67it/s, loss=5.08e+4, metrics={'r2': 0.8855}]\n",
      "epoch 24: 100%|██████████| 1366/1366 [00:20<00:00, 66.26it/s, loss=5.05e+4, metrics={'r2': 0.8867}]\n",
      "epoch 25: 100%|██████████| 1366/1366 [00:21<00:00, 62.55it/s, loss=5.03e+4, metrics={'r2': 0.8872}]\n",
      "epoch 26: 100%|██████████| 1366/1366 [00:21<00:00, 63.44it/s, loss=5.03e+4, metrics={'r2': 0.8876}]\n",
      "epoch 27: 100%|██████████| 1366/1366 [00:21<00:00, 62.44it/s, loss=5.01e+4, metrics={'r2': 0.8887}]\n",
      "epoch 28: 100%|██████████| 1366/1366 [00:22<00:00, 61.69it/s, loss=5.02e+4, metrics={'r2': 0.8881}]\n",
      "epoch 29: 100%|██████████| 1366/1366 [00:20<00:00, 67.57it/s, loss=5e+4, metrics={'r2': 0.8888}]   \n",
      "epoch 30: 100%|██████████| 1366/1366 [00:19<00:00, 69.72it/s, loss=4.99e+4, metrics={'r2': 0.8895}]\n",
      "epoch 31: 100%|██████████| 1366/1366 [00:20<00:00, 66.42it/s, loss=4.97e+4, metrics={'r2': 0.8901}]\n",
      "epoch 32: 100%|██████████| 1366/1366 [00:20<00:00, 66.44it/s, loss=4.99e+4, metrics={'r2': 0.8892}]\n",
      "epoch 33: 100%|██████████| 1366/1366 [00:21<00:00, 64.99it/s, loss=4.97e+4, metrics={'r2': 0.8902}]\n",
      "epoch 34: 100%|██████████| 1366/1366 [00:21<00:00, 63.31it/s, loss=4.96e+4, metrics={'r2': 0.8907}]\n",
      "epoch 35: 100%|██████████| 1366/1366 [00:21<00:00, 63.30it/s, loss=4.96e+4, metrics={'r2': 0.8907}]\n",
      "epoch 36: 100%|██████████| 1366/1366 [00:22<00:00, 61.35it/s, loss=4.95e+4, metrics={'r2': 0.891}] \n",
      "epoch 37: 100%|██████████| 1366/1366 [00:21<00:00, 62.37it/s, loss=4.96e+4, metrics={'r2': 0.8904}]\n",
      "epoch 38: 100%|██████████| 1366/1366 [00:21<00:00, 63.21it/s, loss=4.93e+4, metrics={'r2': 0.8917}]\n",
      "epoch 39: 100%|██████████| 1366/1366 [00:22<00:00, 61.02it/s, loss=4.94e+4, metrics={'r2': 0.8913}]\n",
      "epoch 40: 100%|██████████| 1366/1366 [00:21<00:00, 62.47it/s, loss=4.93e+4, metrics={'r2': 0.8915}]\n",
      "epoch 41: 100%|██████████| 1366/1366 [00:22<00:00, 59.95it/s, loss=4.93e+4, metrics={'r2': 0.892}] \n",
      "epoch 42: 100%|██████████| 1366/1366 [00:19<00:00, 70.11it/s, loss=4.9e+4, metrics={'r2': 0.8932}] \n",
      "epoch 43: 100%|██████████| 1366/1366 [00:19<00:00, 70.25it/s, loss=4.9e+4, metrics={'r2': 0.8931}] \n",
      "epoch 44: 100%|██████████| 1366/1366 [00:21<00:00, 63.61it/s, loss=4.88e+4, metrics={'r2': 0.8938}]\n",
      "epoch 45: 100%|██████████| 1366/1366 [00:21<00:00, 64.23it/s, loss=4.88e+4, metrics={'r2': 0.894}] \n",
      "epoch 46: 100%|██████████| 1366/1366 [00:21<00:00, 63.92it/s, loss=4.9e+4, metrics={'r2': 0.893}]  \n",
      "epoch 47: 100%|██████████| 1366/1366 [00:20<00:00, 66.33it/s, loss=4.87e+4, metrics={'r2': 0.8943}]\n",
      "epoch 48: 100%|██████████| 1366/1366 [00:20<00:00, 66.23it/s, loss=4.88e+4, metrics={'r2': 0.8942}]\n",
      "epoch 49: 100%|██████████| 1366/1366 [00:19<00:00, 69.44it/s, loss=4.89e+4, metrics={'r2': 0.8937}]\n",
      "epoch 50: 100%|██████████| 1366/1366 [00:17<00:00, 78.63it/s, loss=4.85e+4, metrics={'r2': 0.895}] \n",
      "epoch 51: 100%|██████████| 1366/1366 [00:17<00:00, 80.20it/s, loss=4.88e+4, metrics={'r2': 0.8942}]\n",
      "epoch 52: 100%|██████████| 1366/1366 [00:17<00:00, 77.79it/s, loss=4.87e+4, metrics={'r2': 0.8945}]\n",
      "epoch 53: 100%|██████████| 1366/1366 [00:17<00:00, 78.51it/s, loss=4.86e+4, metrics={'r2': 0.895}] \n",
      "epoch 54: 100%|██████████| 1366/1366 [00:17<00:00, 79.66it/s, loss=4.84e+4, metrics={'r2': 0.8955}]\n",
      "epoch 55: 100%|██████████| 1366/1366 [00:17<00:00, 77.69it/s, loss=4.86e+4, metrics={'r2': 0.8951}]\n",
      "epoch 56: 100%|██████████| 1366/1366 [00:18<00:00, 74.33it/s, loss=4.88e+4, metrics={'r2': 0.8938}]\n",
      "epoch 57: 100%|██████████| 1366/1366 [00:18<00:00, 74.89it/s, loss=4.82e+4, metrics={'r2': 0.8966}]\n",
      "epoch 58: 100%|██████████| 1366/1366 [00:21<00:00, 63.32it/s, loss=4.85e+4, metrics={'r2': 0.8954}]\n",
      "epoch 59: 100%|██████████| 1366/1366 [00:22<00:00, 60.61it/s, loss=4.82e+4, metrics={'r2': 0.8963}]\n",
      "epoch 60: 100%|██████████| 1366/1366 [00:25<00:00, 53.80it/s, loss=4.84e+4, metrics={'r2': 0.896}] \n",
      "epoch 61: 100%|██████████| 1366/1366 [00:20<00:00, 66.56it/s, loss=4.83e+4, metrics={'r2': 0.8961}]\n",
      "epoch 62: 100%|██████████| 1366/1366 [00:19<00:00, 71.64it/s, loss=4.8e+4, metrics={'r2': 0.8972}] \n",
      "epoch 63: 100%|██████████| 1366/1366 [00:20<00:00, 66.83it/s, loss=4.82e+4, metrics={'r2': 0.8966}]\n",
      "epoch 64: 100%|██████████| 1366/1366 [00:19<00:00, 70.63it/s, loss=4.81e+4, metrics={'r2': 0.8969}]\n",
      "epoch 65: 100%|██████████| 1366/1366 [00:20<00:00, 65.20it/s, loss=4.8e+4, metrics={'r2': 0.8974}] \n",
      "epoch 66: 100%|██████████| 1366/1366 [00:21<00:00, 62.97it/s, loss=4.8e+4, metrics={'r2': 0.8974}] \n",
      "epoch 67: 100%|██████████| 1366/1366 [00:19<00:00, 70.71it/s, loss=4.8e+4, metrics={'r2': 0.8974}] \n",
      "epoch 68: 100%|██████████| 1366/1366 [00:17<00:00, 76.15it/s, loss=4.8e+4, metrics={'r2': 0.8974}] \n",
      "epoch 69: 100%|██████████| 1366/1366 [00:18<00:00, 73.65it/s, loss=4.79e+4, metrics={'r2': 0.8979}]\n",
      "epoch 70: 100%|██████████| 1366/1366 [00:17<00:00, 75.94it/s, loss=4.8e+4, metrics={'r2': 0.8975}] \n",
      "epoch 71: 100%|██████████| 1366/1366 [00:20<00:00, 65.10it/s, loss=4.79e+4, metrics={'r2': 0.898}] \n",
      "epoch 72: 100%|██████████| 1366/1366 [00:21<00:00, 63.27it/s, loss=4.78e+4, metrics={'r2': 0.8982}]\n",
      "epoch 73: 100%|██████████| 1366/1366 [00:22<00:00, 61.76it/s, loss=4.78e+4, metrics={'r2': 0.8982}]\n",
      "epoch 74: 100%|██████████| 1366/1366 [00:18<00:00, 72.55it/s, loss=4.77e+4, metrics={'r2': 0.8984}]\n",
      "epoch 75: 100%|██████████| 1366/1366 [00:20<00:00, 68.06it/s, loss=4.76e+4, metrics={'r2': 0.8988}]\n",
      "epoch 76: 100%|██████████| 1366/1366 [00:18<00:00, 73.19it/s, loss=4.77e+4, metrics={'r2': 0.8985}]\n",
      "epoch 77: 100%|██████████| 1366/1366 [00:18<00:00, 73.16it/s, loss=4.76e+4, metrics={'r2': 0.8991}]\n",
      "epoch 78: 100%|██████████| 1366/1366 [00:20<00:00, 66.71it/s, loss=4.76e+4, metrics={'r2': 0.8993}]\n",
      "epoch 79: 100%|██████████| 1366/1366 [00:23<00:00, 57.51it/s, loss=4.75e+4, metrics={'r2': 0.8995}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 80: 100%|██████████| 1366/1366 [00:18<00:00, 71.92it/s, loss=4.76e+4, metrics={'r2': 0.8989}]\n",
      "epoch 81: 100%|██████████| 1366/1366 [00:14<00:00, 91.57it/s, loss=4.76e+4, metrics={'r2': 0.8989}]\n",
      "epoch 82: 100%|██████████| 1366/1366 [00:15<00:00, 87.34it/s, loss=4.74e+4, metrics={'r2': 0.8999}]\n",
      "epoch 83: 100%|██████████| 1366/1366 [00:15<00:00, 87.39it/s, loss=4.75e+4, metrics={'r2': 0.8991}] \n",
      "epoch 84: 100%|██████████| 1366/1366 [00:15<00:00, 89.68it/s, loss=4.74e+4, metrics={'r2': 0.8999}]\n",
      "epoch 85: 100%|██████████| 1366/1366 [00:16<00:00, 80.98it/s, loss=4.74e+4, metrics={'r2': 0.9}]   \n",
      "epoch 86: 100%|██████████| 1366/1366 [00:17<00:00, 79.76it/s, loss=4.75e+4, metrics={'r2': 0.8992}] \n",
      "epoch 87: 100%|██████████| 1366/1366 [00:15<00:00, 86.44it/s, loss=4.75e+4, metrics={'r2': 0.8997}]\n",
      "epoch 88: 100%|██████████| 1366/1366 [00:16<00:00, 85.18it/s, loss=4.73e+4, metrics={'r2': 0.9002}] \n",
      "epoch 89: 100%|██████████| 1366/1366 [00:15<00:00, 89.98it/s, loss=4.72e+4, metrics={'r2': 0.9003}] \n",
      "epoch 90: 100%|██████████| 1366/1366 [00:15<00:00, 88.55it/s, loss=4.73e+4, metrics={'r2': 0.9003}]\n",
      "epoch 91: 100%|██████████| 1366/1366 [00:15<00:00, 88.94it/s, loss=4.72e+4, metrics={'r2': 0.9004}]\n",
      "epoch 92: 100%|██████████| 1366/1366 [00:16<00:00, 83.55it/s, loss=4.71e+4, metrics={'r2': 0.901}] \n",
      "epoch 93: 100%|██████████| 1366/1366 [00:17<00:00, 77.88it/s, loss=4.72e+4, metrics={'r2': 0.9007}]\n",
      "epoch 94: 100%|██████████| 1366/1366 [00:19<00:00, 68.39it/s, loss=4.71e+4, metrics={'r2': 0.9012}]\n",
      "epoch 95: 100%|██████████| 1366/1366 [00:19<00:00, 71.14it/s, loss=4.72e+4, metrics={'r2': 0.9006}]\n",
      "epoch 96: 100%|██████████| 1366/1366 [00:20<00:00, 67.24it/s, loss=4.73e+4, metrics={'r2': 0.9003}]\n",
      "epoch 97: 100%|██████████| 1366/1366 [00:21<00:00, 64.66it/s, loss=4.7e+4, metrics={'r2': 0.9016}] \n",
      "epoch 98: 100%|██████████| 1366/1366 [00:20<00:00, 66.68it/s, loss=4.7e+4, metrics={'r2': 0.9016}] \n",
      "epoch 99: 100%|██████████| 1366/1366 [00:21<00:00, 63.28it/s, loss=4.72e+4, metrics={'r2': 0.9008}]\n",
      "epoch 100: 100%|██████████| 1366/1366 [00:21<00:00, 63.02it/s, loss=4.7e+4, metrics={'r2': 0.9013}] \n",
      "predict: 100%|██████████| 1128/1128 [00:07<00:00, 152.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Configuring Features\n",
    "continuous_cols = ['dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality', 'eigenvector_centrality', 'remaining_lease_years', 'floor_area_sqm']\n",
    "categorical_cols = ['month', 'town', 'flat_model_type', 'storey_range']\n",
    "\n",
    "# Step 4: Tab Preprocessing\n",
    "preprocessor = TabPreprocessor(\n",
    "    cat_embed_cols=categorical_cols,\n",
    "    continuous_cols=continuous_cols\n",
    ")\n",
    "\n",
    "# Fit and transform training data, transform test data\n",
    "X_train = preprocessor.fit_transform(train_data.drop(columns=['resale_price']))\n",
    "X_test = preprocessor.transform(test_data.drop(columns=['resale_price']))\n",
    "y_train = train_data['resale_price'].values\n",
    "y_test = test_data['resale_price'].values\n",
    "\n",
    "# Step 5: Create the TabMlp Model\n",
    "model = TabMlp(\n",
    "    column_idx=preprocessor.column_idx,\n",
    "    cat_embed_input=preprocessor.cat_embed_input,\n",
    "    continuous_cols=continuous_cols,\n",
    "    mlp_hidden_dims=[200, 100],\n",
    "    mlp_activation='relu'\n",
    ")\n",
    "\n",
    "# Step 6: Create WideDeep Model\n",
    "wide_deep_model = WideDeep(deeptabular=model)\n",
    "\n",
    "# Step 7: Trainer Configuration\n",
    "trainer = Trainer(\n",
    "    model=wide_deep_model,\n",
    "    objective='rmse',\n",
    "    metrics=[R2Score()],\n",
    "    optimizers=Adam(wide_deep_model.parameters()),\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Step 8: Train the Model\n",
    "trainer.fit(X_tab=X_train, target=y_train, n_epochs=100, batch_size=64)\n",
    "\n",
    "# Step 9: Evaluate the Model on Test Data\n",
    "y_pred = trainer.predict(X_tab=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V46s-MdM0y5c"
   },
   "source": [
    "3.Report the test RMSE and the test R2 value that you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KAhAgvMC07g6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabMlp Model Test RMSE: 102276.53768557492\n",
      "TabMlp Model Test R2: 0.6345309827209406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vaishob\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE & RESULT HERE\n",
    "# Calculate RMSE and R2 on Test Set\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'TabMlp Model Test RMSE: {rmse}')\n",
    "print(f'TabMlp Model Test R2: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
